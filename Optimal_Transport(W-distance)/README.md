
Wasserstein Variational Autoencoder
===============
Wasserstein Variational Autoencoder (W-VAE) is a generative model based on optimal transport (OT) and variational inference. The OT cost is a key to measure the distance between two probability distributions.

**Refrences**
1. Tolstikhin, Ilya, et al. "Wasserstein auto-encoders." arXiv preprint arXiv:1711.01558 (2017).
2. Wasserman, L. "Optimal transport and wasserstein distance." (2017).
3. Chen, Zichuan, and Peng Liu. "Towards Better Data Augmentation using Wasserstein Distance in Variational Auto-encoder." arXiv preprint arXiv:2109.14795 (2021).


Wasserstein distance
---------------
Suppose we have two distributions there are many ways to compute the distance between these two distributions such as KL divergence but there is some drawback with these kinds of distances such as These distances ignore the underlying geometry of the space. Wasserstein distance is useful in practice mainly when the data is supported on low dimensional manifolds in the input space.  Usually, in machine learning, we assume that one's observed data lie on a low-dimensional manifold embedded in a higher-dimensional space. Therefore a distance such as f-divergence which measures the density ratio between distributions often max out and provides no useful gradients for training. 
The $W_2$ distance between two probabilities $p(x)$ and $q(x)$ is defined as follows:
$p(x)\sim N(m_{1},Î£_{1})$
$q(x)\sim N(m_{2},Î£_{2})$

$d = W_{2}(N(m_{1},Î£_{1}),N(m_{2},Î£_{2}))$


$d_{2}=||m_{1}-m{2}||_{2}^{2}+ +Tr(Î£_1+Î£_2-2(Î£_1^{\frac {1}{2}} Î£_2 Î£_1^{\frac{1}{2}})^{\frac{1}{2}})$


Wasserstein Variational Autoencoder
------------------------------------------
In W-VAE we compress the observed data $x$ into low dimentional latent variable $z$ by an aproximate posterior distribution $Q(z|x)$ by the encoder and the decoder reconstruct $p(x)$. The ELBOW lost fucntion in VAE coonsists of mariginal liklihood of the recounstructed data and the KL divergence of the aproximate and true posterior distribution on $z$:
$ELBO_{kl} = E_{z \sim Q(z|x)} [log(p(x|z))] = \int Q(z|x)log(p(x|z))dz -KL(Q(z|x)||p(z))$

For W-VAE we replaced the KL divergence with wasserstein distance :
$ELBO_{W} = E_{z \sim Q(z|x)} [log(p(x|z))] = \int Q(z|x)log(p(x|z))dz -W_p(Q(z|x)||p(z))$

By replacing the $p(z) \sim N(0,1)$ and $Q(z|x) \sim N((Î¼_1, Î¼_2, ...,Î¼_m),{ğœ_12,ğœ_22,â€¦,ğœ_ğ‘š2})$ with  the unknown mean vector ${ğœ‡_1,ğœ‡_2,â€¦,ğœ‡)ğ‘š}$ and diagonal variance vector ${ğœ_12,ğœ_22,â€¦,ğœ_ğ‘š2}$. Since $p(z)$ is multivariate normal we can compute the KL divergence as follows:

$KL(Q(z|x)||p(z)) = \frac {1}{2} [Î _{i=1} ^{m} âˆ’ logğœ_ğ‘– ^2 + Î£_{i=1} ^{m} (ğœ‡_ğ‘– ^2 + ğœ_ğ‘– ^2)âˆ’ğ‘š]$

So to compare the Kl diveregnce with W-distance we have:

$W_2(ğ‘„(ğ‘§|ğ‘¥) || ğ‘ƒ(ğ‘§))=||ğâˆ’ğŸ||_2 ^2+ ğ‘‡ğ‘Ÿ(Î£+ ğˆ âˆ’2(ğˆ^{\frac{1}{2}}Î£ğˆ^{\frac{1}{2}})^{\frac{1}{2}})$

Therefore we can define the difrence disstance between KL divergence and W-distance with $T$ as follows:

$T=W_2(ğ‘„(ğ‘§|ğ‘¥) ||ğ‘ƒ(ğ‘§))âˆ’ KL(ğ‘„(ğ‘§|ğ‘¥) ||ğ‘ƒ(ğ‘§))= ğ‘™ğ‘œğ‘” Î _{i=0} ^m ğœ_ğ‘– ^2 +Î£_{i=1} ^m (ğœ_ğ‘š âˆ’ 2)^2 + Î£_{i=1} ^m ğœ‡_ğ‘–^2âˆ’ğ‘š$

The value of $T$ depends on the variances $ğœ_ğ‘–$

1. When $ğœ_1,ğœ_2,â€¦,ğœ_ğ‘š=1$, $T=0$, suggesting that ELBOW and ELBOKL are identical at this point.
2. When $ğœ_1,ğœ_2,â€¦,ğœ_ğ‘šâ‰¤1$, itâ€™s easy to show $\frac{ğœ•T}{ğœ_1},\frac{ğœ•T}{ğœ_2},â€¦,\frac{ğœ•T}{ğœ_m}â‰¥0$, suggesting that $T$ increases monotonically across ${ğœ_1,ğœ_2,â€¦,ğœ_ğ‘š}$, jointly resulting in $Tâ‰¤0$.

Therefore, when $ğœ_1,ğœ_2,â€¦,ğœ_ğ‘šâ‰¤1$, ELBOW is closer to $log(ğ‘ƒ(ğ‘¥))$ than ELBOKL in its approximation.
ELBOW may not be a consistent estimator when the model overfits, and the training process may conclude with a higher ELBOW than $log(ğ‘ƒ(ğ‘¥))$. To enforce the consistency property of ELBOW, it is necessary to place an inductive bias such that $T=0$ upon model convergence. This is achieved by introducing an additional hyperparameter $ğœ†$ that controls the weight of the Wasserstein distance term, forming a new objective function $ELBOW_Î»$ as follows:

$ELBOW_Î»= âˆ«ğ‘„(ğ‘§|ğ‘¥)log(ğ‘ƒ(ğ‘¥|ğ‘§))ğ‘‘ğ‘§ âˆ’ Î» W_2(ğ‘„(ğ‘§|ğ‘¥) || ğ‘ƒ(ğ‘§))$









